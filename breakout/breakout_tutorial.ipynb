{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "save picture -- Breakout_random.gif\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()\n",
    "done = False\n",
    "frames_random = []\n",
    "i = 0\n",
    "while not done:\n",
    "    img = env.render(mode='rgb_array')[32:190,8:152]\n",
    "    img = Image.fromarray(img)\n",
    "    frames_random.append(img)\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    i = i+1\n",
    "print(i)\n",
    "frames_random[0].save('Breakout_random.gif', format='GIF', append_images=frames_random[1:], save_all=True, duration=0.0001)\n",
    "print(\"save picture -- Breakout_random.gif\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DQN reinforce learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : \n",
    "1. https://passi0n.tistory.com/88\n",
    "2. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from PIL import Image, ImageDraw\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, init_dim, layers, class_num, device):\n",
    "        super(myDQN, self).__init__()\n",
    "        input_dim = init_dim\n",
    "        linear_layers = nn.ModuleList()\n",
    "        for dim in layers:\n",
    "            linear_layers.append(nn.Linear(input_dim, dim).to(device))\n",
    "            linear_layers.append(nn.LeakyReLU())\n",
    "            input_dim = dim\n",
    "        linear_layers.append(nn.Linear(input_dim, class_num).to(device))\n",
    "        self.linear_layers = linear_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linear_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class cnnDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, resize_unit, history_size, outputs, device):\n",
    "        h, w = resize_unit\n",
    "        super(cnnDQN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(history_size, 16, kernel_size=5, stride=2).to(device)\n",
    "        self.bn1 = nn.BatchNorm2d(16).to(device)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2).to(device)\n",
    "        self.bn2 = nn.BatchNorm2d(32).to(device)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2).to(device)\n",
    "        self.bn3 = nn.BatchNorm2d(32).to(device)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs).to(device)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    \n",
    "class memoryDataset(object):\n",
    "    def __init__(self, maxlen, device):\n",
    "        self.memory = deque(maxlen=maxlen)\n",
    "        self.subset = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done', 'life'))\n",
    "        self.device = device\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done, life):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        action = torch.tensor([action], dtype=torch.long).to(self.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float).to(self.device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float).to(self.device)\n",
    "        ##False:0, True:1\n",
    "        done = torch.tensor([done], dtype=torch.long).to(self.device)\n",
    "        ##Life : 0,1,2,3,4,5\n",
    "        life = torch.tensor(life, dtype=torch.float).to(self.device)\n",
    "        self.memory.append(self.subset(state, action, reward, next_state, done, life))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        batch = self.subset(*zip(*batch))\n",
    "        return batch\n",
    "    \n",
    "class myHistory(object):\n",
    "    def __init__(self, history_size, state, device):\n",
    "        assert history_size>1, \"history_size should be more than 1\"\n",
    "        self.history_size = history_size\n",
    "        self.device = device\n",
    "        temp = []\n",
    "        for _ in range(history_size):\n",
    "            temp.append(state)\n",
    "        self.history = torch.stack(temp)\n",
    "        \n",
    "    def push(self, state):\n",
    "        self.history = torch.cat([self.history, state.unsqueeze(0)])[1:]\n",
    "        \n",
    "    def pull(self):\n",
    "        return self.history\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver():\n",
    "    def __init__(self, n_episodes=10000, max_env_steps=None, gamma=1.0, \n",
    "                 epsilon=1.0, epsilon_min=0.01, class_num=4, epsilon_log_decay=0.995, \n",
    "                 alpha=0.01, alpha_decay=0.01, batch_size=4, monitor=True, quiet=False, device='cpu', \n",
    "                 pretrained=None, rendering=False, history_size=2, resize_unit=(161, 144)):\n",
    "        self.device = device\n",
    "        self.memory = memoryDataset(maxlen=100000, device=device)\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        self.n_episodes = n_episodes\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        self.class_num = class_num\n",
    "        self.model = cnnDQN(resize_unit, history_size, class_num, self.device)\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=alpha, weight_decay=alpha_decay)\n",
    "        self.resize_unit = resize_unit\n",
    "        self.history_size = history_size\n",
    "        \n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        save_folder = os.path.join(os.getcwd(), 'model')\n",
    "        if not os.path.isdir(save_folder):\n",
    "            os.mkdir(save_folder)\n",
    "        self.save_path = os.path.join(save_folder, 'model.pkl')\n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon=None):\n",
    "        if epsilon is not None and np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = state.unsqueeze(0).to(self.device)\n",
    "                action = self.model(state) if self.device =='cpu' else self.model(state).cpu()\n",
    "                return int(action.max(dim=1).indices.numpy())\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        state = torch.stack(batch.state)\n",
    "        action = torch.stack(batch.action)\n",
    "        next_state = torch.stack(batch.next_state)\n",
    "        reward = torch.stack(batch.reward)\n",
    "        done = torch.stack(batch.done)\n",
    "        life = torch.stack(batch.life)\n",
    "        with torch.no_grad():\n",
    "            next_state_action_values = self.model(next_state)\n",
    "        next_state_value = torch.max(next_state_action_values, dim=1).values.view(-1, 1)\n",
    "        reward = reward.view(-1, 1)\n",
    "        target_state_value = torch.stack([reward + (self.gamma * next_state_value), reward], dim=1).squeeze().gather(1, done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        state_action_values = self.model(state).gather(1, action)\n",
    "        loss = F.mse_loss(state_action_values, target_state_value)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def init_history(self, state):\n",
    "        temp = []\n",
    "        for _ in range(self.history_size):\n",
    "            temp.append(state)\n",
    "        return torch.stack(temp)\n",
    "    \n",
    "    def push_history(self, history, state):\n",
    "        return torch.cat([history, state.unsqueeze(0)])[1:]\n",
    "    \n",
    "    def convert_channel(self, img):\n",
    "        # input type : |img| = (Height, Width, channel)\n",
    "\n",
    "        #remove useless item\n",
    "        img = img[32:193,8:152]\n",
    "\n",
    "        #conver channel(3) -> channel(1)\n",
    "        img = np.any(img, axis=2)\n",
    "        # |img| = (Height, Width)  boolean\n",
    "        return torch.tensor(img, dtype=torch.float)\n",
    "        \n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "        lives = deque(maxlen=100)\n",
    "        for episode in range(self.n_episodes):\n",
    "            score = 0\n",
    "            state = self.env.reset()\n",
    "            state = self.convert_channel(state)\n",
    "            history = self.init_history(state)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(history, self.get_epsilon(episode))\n",
    "                next_state, reward, done, life = self.env.step(action)\n",
    "                next_state = self.convert_channel(next_state)\n",
    "                life = life['ale.lives']\n",
    "                next_history = self.push_history(history, next_state)\n",
    "                self.memory.push(history, action, reward, next_history, done, life)\n",
    "                history = next_history\n",
    "                score += reward\n",
    "\n",
    "            scores.append(score)\n",
    "            lives.append(life)\n",
    "            mean_score = np.mean(scores) ##최근 100개가 버티는 시간의 Mean이 조건을 만족하면 멈춤..\n",
    "            mean_life = np.mean(lives)\n",
    "            if life > 0 and done:\n",
    "                if not self.quiet: print('{} episodes. Solved after {} trials '.format(episode, episode - 100))\n",
    "                SAVE_PATH = '/model/model.pkl'\n",
    "                self.save_model()\n",
    "\n",
    "                return episode - 100\n",
    "            if episode % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - last 100 episodes score : {}, life : {}, epsilon : {}'.format(episode, mean_score, mean_life, self.epsilon))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes'.format(episode))\n",
    "        return episode\n",
    "\n",
    "    def save_model(self):\n",
    "        param_groups= {}\n",
    "        param_groups['model_state_dict'] = self.model.state_dict()\n",
    "        param_groups['optimizer_state_dict'] = self.optimizer.state_dict()\n",
    "        #param_groups['class_num'] = self.class_num\n",
    "        param_groups['gamma'] = self.gamma\n",
    "        param_groups['epsilon'] = self.epsilon\n",
    "        param_groups['alpha'] = self.alpha\n",
    "        #param_groups['alpha_decay'] = self.alpha_decay\n",
    "        param_groups['batch_size'] = self.batch_size\n",
    "        torch.save(param_groups, self.save_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        checkpoint = torch.load(self.save_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    def render_policy_net(self):\n",
    "        self.load_model()\n",
    "        #self.env = gym.wrappers.Monitor(self.env, os.path.join(os.getcwd(), 'model','cartpole-1'), force=True)\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        frames = []\n",
    "        raw_frames = []\n",
    "        i = 0\n",
    "        while not done:\n",
    "            img = self.env.render(mode='rgb_array')\n",
    "            raw_frames.append(img)\n",
    "            img = Image.fromarray(img)\n",
    "            frames.append(img)\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "            i = i+1\n",
    "        print(i)\n",
    "        self.env.close()\n",
    "        frames[0].save('Breakout_result.gif', format='GIF', append_images=frames[1:], save_all=True, duration=0.0001)\n",
    "        print(\"save picture -- Breakout_result.gif\")\n",
    "        return frames, raw_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\program files\\python37\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - last 100 episodes score : 0.0, life : 0.0, epsilon : 1.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-d8aab914da3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#frames, raw_frames = agent.render_policy_net()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-c4e0f28e17e5>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Episode {}] - last 100 episodes score : {}, life : {}, epsilon : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_life\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Did not solve after {} episodes'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-183-c4e0f28e17e5>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_state_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:62] data. DefaultCPUAllocator: not enough memory: you tried to allocate %dGB. Buy new RAM!0\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "agent = DQNSolver(device=device)\n",
    "agent.run()\n",
    "#frames, raw_frames = agent.render_policy_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()\n",
    "done = False\n",
    "frames_random = []\n",
    "i = 0\n",
    "samples = []\n",
    "for i in range(10):\n",
    "    img = env.render(mode='rgb_array')\n",
    "    samples += [img]\n",
    "    img = Image.fromarray(img)\n",
    "    frames_random.append(img)\n",
    "    \n",
    "    img_name = str(i) + \".bmp\"\n",
    "    img.save(img_name)\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, life = env.step(action)\n",
    "    state = next_state\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life['ale.lives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def convert_channel(img):\n",
    "    # input type : |img| = (Height, Width, channel)\n",
    "    \n",
    "    #remove useless item\n",
    "    img = img[32:190,8:152]\n",
    "    \n",
    "    #conver channel(3) -> channel(1)\n",
    "    img = np.any(img, axis=2)\n",
    "    # |img| = (Height, Width)  boolean\n",
    "    return torch.tensor(img, dtype=torch.int)\n",
    "\n",
    "def get_import_value(imgs):\n",
    "    #Block are beside 25:61 Height\n",
    "    #Bar are in last Height\n",
    "    \n",
    "    assert len(imgs)>1, \"This module need more than 1 image\"\n",
    "    output = np.array([])\n",
    "    \n",
    "    #Set Bar of last Picture\n",
    "    output = np.append(np.mean(np.where(imgs[-1][-1]))) ##bar index\n",
    "    \n",
    "    #Set Block that still exist\n",
    "    output = np.append(np.all(imgs[:][25:61], axis=0).reshape(-1))\n",
    "    temp = np.ones(len(imgs[0][25:61].reshape(-1)))\n",
    "    for img in imgs:\n",
    "        temp = np.all([temp,img[25:61].reshape(-1)], axis=0)\n",
    "        \n",
    "    #Find Ball that move\n",
    "    \n",
    "    # It is possible to check Ball position by checking difference of view\n",
    "    for idx in range(len(imgs)-1):\n",
    "        diff = np.where(np.diff([img1[:-1]>img2[:-1]]))\n",
    "        output += np.mean(dff[1])\n",
    "        output += np.mean(dff[2])\n",
    "        \n",
    "        diff = np.where(np.diff([img1[:-1]<img2[:-1]]))\n",
    "        output += np.mean(dff[1])\n",
    "        output += np.mean(dff[2])\n",
    "        \n",
    "    # Block\n",
    "    \n",
    "        \n",
    "    output = np.append(output, temp)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [200,  72,  72],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0],\n",
       "        [  0,   0,   0]]], dtype=uint8)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[7][192:193,8:152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "imgs.append(convert_channel(samples[7]))\n",
    "imgs.append(convert_channel(samples[8]))\n",
    "imgs.append(convert_channel(samples[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210, 160, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(samples[7]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack([torch.tensor([1,1]), torch.tensor([2,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = torch.cat([a, torch.tensor([3,3]).unsqueeze(0)])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
